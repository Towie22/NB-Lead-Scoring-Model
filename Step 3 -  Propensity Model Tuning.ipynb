{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633efa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "!{sys.executable} -m pip install hyperopt\n",
    "!{sys.executable} -m pip install ipython-autotime\n",
    "!{sys.executable} -m pip install pandas-profiling\n",
    "!{sys.executable} -m pip install joblib\n",
    "!{sys.executable} -m pip install pdpbox\n",
    "!{sys.executable} -m pip install optuna\n",
    "!{sys.executable} -m pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action = 'ignore', category = SettingWithCopyWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from config import *\n",
    "from utils import *\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "sns.set(rc={'figure.figsize':(16,8)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dac477",
   "metadata": {},
   "source": [
    "## Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19752ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILEPATH = './Data/Prop_Data/train'\n",
    "TEST_FILEPATH = './Data/Prop_Data/test'\n",
    "\n",
    "# Train Paths\n",
    "X_train = pd.read_csv('{}/train.csv'.format(TRAIN_FILEPATH))\n",
    "X_train2 = pd.read_csv('{}/train2.csv'.format(TRAIN_FILEPATH))\n",
    "X_train3 = pd.read_csv('{}/train3_label.csv'.format(TRAIN_FILEPATH))\n",
    "\n",
    "y_train = pd.read_csv('{}/train_labels.csv'.format(TRAIN_FILEPATH))\n",
    "y_train2 = pd.read_csv('{}/train2_label.csv'.format(TRAIN_FILEPATH))\n",
    "y_train3 = pd.read_csv('{}/train3_label.csv'.format(TRAIN_FILEPATH))\n",
    "\n",
    "# Test Paths\n",
    "X_test = pd.read_csv('{}/test.csv'.format(TEST_FILEPATH))\n",
    "X_calibrated = pd.read_csv('{}/test_labels.csv'.format(TEST_FILEPATH))\n",
    "X_val = pd.read_csv('{}/validation.csv'.format(TEST_FILEPATH))\n",
    "\n",
    "y_test = pd.read_csv('{}/test_labels.csv'.format(TEST_FILEPATH))\n",
    "y_calibrated = pd.read_csv('{}/calibrated_labels.csv'.format(TEST_FILEPATH))\n",
    "y_val = pd.read_csv('{}/validation_label.csv'.format(TEST_FILEPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f197226",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model_results/prop_model'\n",
    "\n",
    "nb_model = joblib.load('{}/prop_model_hyperopt_20211008.pkl'.format(model_path))\n",
    "Hyperopt_model = load_model.fit(X_train2[[i for i in df_cleaned.columns if i in FEATURES]], y_train2)\n",
    "Optuna_model = load_model.fit(X_train2[[i for i in df_cleaned.columns if i in FEATURES]], y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d188e",
   "metadata": {},
   "source": [
    "# HyperOpts Tuning Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.tpe import suggest\n",
    "from hyperopt.pyll.stochastic import sample as ho_sample\n",
    "from sklearn.model_selection import KFold,TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyper tuning parameters and value range\n",
    "hyperopt_space = {\n",
    "    'base_score': np.mean(X_train3[LABEL]),\n",
    "    'objective': 'binary:logistic',\n",
    "    'n_estimators': hp.choice('n_estimators',[400,600,800]),\n",
    "    'max_depth': hp.choice('max_depth', [4,5,6]),\n",
    "    'learning_rate': hp.loguniform('learning_rate', low = -2 * np.log(10), high = -1 * np.log(10)),\n",
    "    'gamma': hp.uniform('gamma', 0.01, .7),\n",
    "    'subsample': hp.quniform('subsample', 0.7, 1, 0.1),\n",
    "    'alpha': hp.quniform('alpha', 0.5, 1.5, 0.1),\n",
    "    'lambda': hp.quniform('lambda', 0.5, 1.5, 0.1),\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.7, 1, 0.1),\n",
    "    'max_delta_step': hp.quniform('max_delta_step',1, 10, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', low =0, high = 20),\n",
    "#     'scale_pos_weight': hp.loguniform('scale_pos_weight', low = 0, high = 5),\n",
    "#     'eval_metric': 'auc'\n",
    "     'eval_metric':'aucpr'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Hyperopts framework to get best parameters\n",
    "%%time\n",
    "def hyperparameter_tuning(param):\n",
    "    xgb_model = xgb.XGBClassifier(**param, random_state = 0)\n",
    "    model = xgb_model.fit(X_train3[[i for i in df_cleaned.columns if i in FEATURES]], y_train3)\n",
    "    \n",
    "    preds = xgb_model.predict(X_val[model.get_booster().feature_names] )\n",
    "    \n",
    "    f1 = f1_score(y_val, preds)\n",
    "    \n",
    "    return 1 - f1\n",
    "\n",
    "trials = Trials()\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "param = hyperopt_space\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    hyperopt_results = fmin(\n",
    "        fn = hyperparameter_tuning,\n",
    "        space = hyperopt_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50,\n",
    "        trials=trials,\n",
    "        rstate = random_state\n",
    "    )\n",
    "        \n",
    "\n",
    "print(\"Best: {}\".format(hyperopt_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d96c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(obj):\n",
    "   for attr in dir(obj):\n",
    "       if hasattr( obj, attr ):\n",
    "            \n",
    "            trial_obj = obj.__dir__\n",
    "            print( \"obj.%s = %s\" % (attr, getattr(obj, attr)))\n",
    "            \n",
    "            \n",
    "\n",
    "t = dump(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Model with outputted parameters\n",
    "\n",
    "%%time\n",
    "# Grab the best hyperparaemeters from the best trial\n",
    "# This code should ideally be synced to the hyperparameter grid \n",
    "best_hyperparams = {\n",
    "    'n_estimators': [200,400,600,800][trials.best_trial['misc']['vals']['n_estimators'][0]],\n",
    "    'max_depth': [4,5,6][trials.best_trial['misc']['vals']['max_depth'][0]],\n",
    "    'learning_rate': trials.best_trial['misc']['vals']['learning_rate'][0],\n",
    "    'subsample': trials.best_trial['misc']['vals']['subsample'][0],\n",
    "    'gamma': trials.best_trial['misc']['vals']['gamma'][0],\n",
    "    'alpha': trials.best_trial['misc']['vals']['alpha'][0],\n",
    "    'lambda': trials.best_trial['misc']['vals']['lambda'][0],\n",
    "    'colsample_bytree': trials.best_trial['misc']['vals']['colsample_bytree'][0],\n",
    "    'max_delta_step': trials.best_trial['misc']['vals']['max_delta_step'][0],\n",
    "    'scale_pos_weight': trials.best_trial['misc']['vals']['scale_pos_weight'][0],\n",
    "    'eval_metric': 'aucpr'\n",
    "}\n",
    "\n",
    "best_model = xgb.XGBClassifier(**best_hyperparams, random_state = 0)\n",
    "best_model.fit(X_train[[i for i in df_cleaned.columns if i in FEATURES]], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d361b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model.predict(X_test[[i for i in df_cleaned.columns if i in FEATURES]][best_model.get_booster().feature_names])\n",
    "pred_proba = best_model.predict_proba(X_test[[i for i in df_cleaned.columns if i in FEATURES]][best_model.get_booster().feature_names])[:,1]\n",
    "\n",
    "acc = round(accuracy_score(y_test, preds), 3)\n",
    "auc = round(roc_auc_score(y_test, pred_proba), 3)\n",
    "precision = round(precision_score(y_test, preds), 3)\n",
    "recall = round(recall_score(y_test, preds), 3)\n",
    "f1 = round(f1_score(y_test, preds), 3)\n",
    "\n",
    "print('Model evaluation: ACC: {} / AUC: {} /  Precision: {} / Recall: {} / F1: {}'.format(acc, auc, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df507798",
   "metadata": {},
   "source": [
    "## Save Hyperopts Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trials\n",
    "HYPEROPT_OUTPUT_PATH = './model_results/prop_model'\n",
    "MODEL_NAME = 'prop_trial_hyperopt1'\n",
    "DATE = '_20210923'\n",
    "hyp_model = joblib.dump(best_model,  '{}/{}{}{}'.format(HYPEROPT_OUTPUT_PATH,MODEL_NAME,DATE, '_trials.pkl'))\n",
    "hyp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3ec69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e2b9505",
   "metadata": {},
   "source": [
    "# Optuna Tuning Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0237ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    param_space = {\n",
    "        'base_score': np.mean(X_train3[LABEL]),\n",
    "        'objective': 'binary:logistic',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10,100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4,6),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', low = 0.1, high = 1),\n",
    "        'gamma': trial.suggest_uniform('gamma', 1e-8, 1),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.7, 1),\n",
    "        'alpha': trial.suggest_uniform('alpha', 0.5, 1.5),\n",
    "        'lambda': trial.suggest_uniform('lambda', 0.5, 1.5),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1),\n",
    "        'max_delta_step': trial.suggest_uniform('max_delta_step', 1, 10),\n",
    "        'scale_pos_weight': trial.suggest_loguniform('scale_pos_weight', 0.001, 20),\n",
    "        'eval_metric': 'auc'\n",
    "        }\n",
    "    \n",
    "    \n",
    "    model = xgb.XGBClassifier(**param_space)\n",
    "    \n",
    "    model.fit(X_train3[[i for i in df_cleaned.columns if i in FEATURES]], y_train3)\n",
    "    \n",
    "    pred = model.predict(X_val[nb_model.get_booster().feature_names])\n",
    "    f1 = f1_score(y_val, pred)\n",
    "    \n",
    "    return(f1)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print('Best Parameters:', study.best_params)\n",
    "print()\n",
    "print('Best Value:', study.best_value)\n",
    "print()\n",
    "print('Best Trial:', study.best_trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80239a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = xgb.XGBClassifier(**study.best_params)\n",
    "test_model.fit(X_train[[i for i in df_cleaned.columns if i in FEATURES]], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_preds = test_model.predict(X_test[[i for i in df_cleaned.columns if i in FEATURES]][test_model.get_booster().feature_names])\n",
    "opt_pred_proba = test_model.predict_proba(X_test[[i for i in df_cleaned.columns if i in FEATURES]][test_model.get_booster().feature_names])[:,1]\n",
    "\n",
    "acc = round(accuracy_score(y_test, opt_preds), 3)\n",
    "auc = round(roc_auc_score(y_test, opt_pred_proba), 3)\n",
    "precision = round(precision_score(y_test, opt_preds), 3)\n",
    "recall = round(recall_score(y_test, opt_preds), 3)\n",
    "f1 = round(f1_score(y_test, opt_preds), 3)\n",
    "\n",
    "print('Model evaluation: ACC: {} / AUC: {} /  Precision: {} / Recall: {} / F1: {}'.format(acc, auc, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b052a",
   "metadata": {},
   "source": [
    "## Save Optuna Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trials\n",
    "OPTUNA_OUTPUT_PATH = './model_results/prop_model'\n",
    "MODEL_NAME = 'prop_trial_optuna'\n",
    "DATE = '_20210923'\n",
    "opt_model = joblib.dump(test_model,  '{}/{}{}{}'.format(OPTUNA_OUTPUT_PATH,MODEL_NAME,DATE, '_trials.pkl'))\n",
    "opt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990beb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bef02b3f",
   "metadata": {},
   "source": [
    "# Model Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.calibration import calibration_curve\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8c319",
   "metadata": {},
   "source": [
    "## Load Best Model for Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5dbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = joblib.load('{}/prop_model_hyperopt_20211008.pkl'.format(model_path))\n",
    "model = load_model.fit(X_train2[[i for i in df_cleaned.columns if i in FEATURES]], y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a920e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test[model.get_booster().feature_names])\n",
    "pred_proba = model.predict_proba(X_test[model.get_booster().feature_names])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d35271",
   "metadata": {},
   "source": [
    "## \"Sigmoid\" Model Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8bb911",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated = CalibratedClassifierCV(model, method='sigmoid', cv=3)\n",
    "calibrated.fit(X_calibrated[[i for i in df_cleaned.columns if i in FEATURES]], y_calibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_pred_train = calibrated.predict(X_train[model.get_booster().feature_names])\n",
    "sig_pred_proba_train = calibrated.predict_proba(X_train[model.get_booster().feature_names])[:,1]\n",
    "\n",
    "sig_pred = calibrated.predict(X_test[model.get_booster().feature_names])\n",
    "sig_pred_proba = calibrated.predict_proba(X_test[model.get_booster().feature_names])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration Plot\n",
    "\n",
    "# predict probabilities\n",
    "probs = calibrated.predict_proba(X_test[[i for i in df_cleaned.columns if i in FEATURES]])[:, 1]\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(y_test, probs, n_bins=10, normalize=True)\n",
    "# plot perfectly calibrated\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot calibrated reliability\n",
    "pyplot.plot(mpv, fop, marker='.')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PP Plot\n",
    "\n",
    "X_test['predictions'] = sig_pred_proba\n",
    "X_test['label'] = y_test\n",
    "decile_plot(X_test, 'predictions', 'label', 'with product')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce265c",
   "metadata": {},
   "source": [
    "## \"Isotonic\" Model Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4625418",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated = CalibratedClassifierCV(model, method='isotonic', cv=3)\n",
    "calibrated.fit(X_calibrated[[i for i in df_cleaned.columns if i in FEATURES]], y_calibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02daea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_pred_train = calibrated.predict(X_train[model.get_booster().feature_names])\n",
    "iso_pred_proba_train = calibrated.predict_proba(X_train[model.get_booster().feature_names])[:,1]\n",
    "\n",
    "iso_pred = calibrated.predict(X_test[model.get_booster().feature_names])\n",
    "iso_pred_proba = calibrated.predict_proba(X_test[model.get_booster().feature_names])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration Plot\n",
    "\n",
    "# predict probabilities\n",
    "probs = calibrated.predict_proba(X_test[[i for i in df_cleaned.columns if i in FEATURES]])[:, 1]\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(y_test, probs, n_bins=10, normalize=True)\n",
    "# plot perfectly calibrated\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot calibrated reliability\n",
    "pyplot.plot(mpv, fop, marker='.')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PP Plot\n",
    "\n",
    "X_test['predictions'] = sig_pred_proba\n",
    "X_test['label'] = y_test\n",
    "decile_plot(X_test, 'predictions', 'label', 'with product')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
