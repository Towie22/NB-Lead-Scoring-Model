{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "!{sys.executable} -m pip install ipython-autotime\n",
    "!{sys.executable} -m pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action = 'ignore', category = SettingWithCopyWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from config import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfdf8e8",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Data\"\n",
    "\n",
    "df_raw = pd.read_csv('{}/nb_purchase_propensity_model_raw_data.csv000'.format(path))\n",
    "df_policy = pd.read_csv('{}/nb_purchase_propensity_model_raw_data_compliment_view1.csv000'.format(path))\n",
    "df_quote = pd.read_csv('{}/nb_purchase_propensity_model_raw_data_compliment_view2.csv000'.format(path))\n",
    "\n",
    "def nric_check(row):\n",
    "        #     print(type(row['nric']))\n",
    "        if len(str(row['nric'])) == 9 and str(row['nric'])[:1] in ['S', 'T']:\n",
    "            return 'SGPR'\n",
    "        elif len(str(row['nric'])) == 9 and str(row['nric'])[:1] in ['F', 'G']:\n",
    "            return 'FOREIGNER'\n",
    "        else:\n",
    "            return 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_quote = df_raw.merge(df_quote, left_on='quote_origin_system_number', right_on='current_quote_origin_system_number', how='left', indicator=True)\n",
    "df_raw_quote = df_raw_quote.rename(columns={\"_merge\": \"matched_quote\"})\n",
    "df_combined = df_raw_quote.merge(df_policy, left_on='quote_origin_system_number', right_on='current_quote_origin_system_number', how='left', indicator=True)\n",
    "df_combined = df_combined.rename(columns={\"_merge\": \"matched_policy\"})\n",
    "df_combined = df_combined.drop(columns=['product_type_y', 'current_quote_origin_system_number_x', 'current_quote_origin_system_number_y'])\n",
    "df_combined = df_combined.rename(columns={\"applicant_nric_x\": \"nric\", \"applicant_nric_y\": \"nric_quote\", \"individual_id_number\": \"nric_policy\", \"product_type_x\": \"product_type\",})\n",
    "df_combined['nric_type'] = df_combined.apply(nric_check, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22796d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates\n",
    "df_combined['applicant_dob']= pd.to_datetime(df_combined['applicant_dob'], errors = 'coerce')\n",
    "df_combined['age_at_quote'] = np.floor((df_combined['quote_issue_date'] - df_combined['applicant_dob'])/np.timedelta64(1, 'Y'))\n",
    "df_combined['update_timediff'] = np.floor((df_combined['quote_latest_update_date'] - df_combined['quote_issue_date'])/np.timedelta64(1, 'm'))\n",
    "df_combined['conversion_timediff'] = np.floor((df_combined['policy_issue_date'] - df_combined['quote_issue_date'])/np.timedelta64(1, 'D'))\n",
    "df_combined['since_first_quote'] = np.floor((df_combined['quote_issue_date'] - df_combined['first_quote_created_before_date'])/np.timedelta64(1, 'W'))\n",
    "df_combined['since_latest_quote'] = np.floor((df_combined['quote_issue_date'] - df_combined['latest_quote_created_before_date'])/np.timedelta64(1, 'W'))\n",
    "df_combined['since_first_policy'] = np.floor((df_combined['quote_issue_date'] - df_combined['scv_first_policy_issue_date'])/np.timedelta64(1, 'W'))\n",
    "df_combined['since_latest_policy'] = np.floor((df_combined['quote_issue_date'] - df_combined['scv_latest_policy_issue_date'])/np.timedelta64(1, 'W'))\n",
    "df_combined['since_first_interaction'] = np.floor((df_combined['first_interaction_date'] - df_combined['quote_issue_date'])/np.timedelta64(1, 'h'))\n",
    "df_combined['since_last_interaction'] = np.floor((df_combined['last_interaction_date'] - df_combined['quote_issue_date'])/np.timedelta64(1, 'h'))\n",
    "df_combined['update_flag'] = np.where(np.floor((df_combined['quote_latest_update_date'] - df_combined['quote_issue_date'])/np.timedelta64(1, 'm')) > 0, 1, 0)\n",
    "df_combined['multiple_interaction_flag'] = np.where(np.floor((df_combined['last_interaction_date'] - df_combined['first_interaction_date'])/np.timedelta64(1, 's')) > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call counts\n",
    "df_combined['customer_request_call_count'] = df_combined['customer_request_call_count'].fillna(0)\n",
    "df_combined['customer_request_no_pick_up_count'] = df_combined['customer_request_no_pick_up_count'].fillna(0)\n",
    "df_combined['customer_request_call_attempt_count'] = (df_combined['customer_request_call_count'] + df_combined['customer_request_no_pick_up_count']).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bd895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unknown NRICs\n",
    "df_cleaned = df_combined[df_combined['nric_type'] != 'UNKNOWN']\n",
    "\n",
    "# removing STP\n",
    "df_cleaned = df_cleaned[df_cleaned['stp_flag'] == 0]\n",
    "\n",
    "# removing issued before interaction\n",
    "df_cleaned = df_cleaned[df_cleaned['issued_before_interaction_flag'] == 0]\n",
    "\n",
    "# removing assisted quotes\n",
    "df_cleaned = df_cleaned[df_cleaned['assisted_call_flag'] == \"Outbound - Assisted\"]\n",
    "\n",
    "# consolidating product types\n",
    "mapping_product = {'Heart Attack Insurance': 'Big 3 Critical Illness', 'Term Life Plus': 'Life', 'Term Life Plus': 'Life', 'DPI': 'Life', 'Cancer': 'Big 3 Critical Illness',\n",
    "                   'TermLife': 'Life', 'Heart Attack Insurance-Stroke Insurance': 'Big 3 Critical Illness', 'Stroke Insurance': 'Big 3 Critical Illness', \n",
    "                   'Cancer-Heart Attack Insurance': 'Big 3 Critical Illness', 'Cancer-Heart Attack Insurance-Stroke Insurance': 'Big 3 Critical Illness', \n",
    "                   'Essential Life': 'Life', 'Cancer-Stroke Insurance': 'Big 3 Critical Illness', 'Modular Maid - 6ME Homebased-Modular Maid - OutPatient': 'Maid', \n",
    "                   'Modular Maid - OutPatient': 'Maid', 'Modular Maid - 6ME Clinics': 'Maid', 'Modular Maid - POLO': 'Maid', 'Modular Maid - 6ME Homebased': 'Maid', \n",
    "                   'Modular Maid - 6ME Clinics-Modular Maid - OutPatient': 'Maid', 'Modular Maid - 6ME Homebased-Modular Maid - POLO': 'Maid', \n",
    "                   'Modular Maid - 6ME Homebased-Modular Maid - OutPatient-Modular Maid - POLO': 'Maid', 'Modular Maid - 6ME Clinics-Modular Maid - POLO': 'Maid', \n",
    "                   'Modular Maid - 6ME Clinics-Modular Maid - OutPatient-Modular Maid - POLO': 'Maid', 'Modular Maid - 6ME Homebased-Modular Maid - OutPatient': 'Maid', \n",
    "                   'Modular Maid - OutPatient-Modular Maid - POLO': 'Maid'}\n",
    "df_cleaned['product_type'].replace(mapping_product, inplace=True)\n",
    "\n",
    "# consolidating genders\n",
    "mapping_gender = {'Male': 'M', 'm': 'M', 'male': 'M', 'Female': 'F', 'f': 'F', 'Femaile': 'F', 'u': 'U'}\n",
    "df_cleaned['applicant_gender'].replace(mapping_gender, inplace=True)\n",
    "df_cleaned['applicant_gender'] = df_cleaned['applicant_gender'].fillna('U')\n",
    "\n",
    "# consolidating nationalities, incomplete\n",
    "mapping_nationality = {'Singapore': 'SGP', 'Malaysia': 'MYS'}\n",
    "df_cleaned['applicant_nationality'].replace(mapping_nationality, inplace=True)\n",
    "\n",
    "# removed extreme promo code value (100,000)\n",
    "df_cleaned = df_cleaned[df_cleaned['quotation_promo_code_value'] != 100000]\n",
    "\n",
    "# consolidating distribution_channel\n",
    "mapping_distribution = {'AFFINITY': \"AFFILIATE\", 'SOCIAL': \"REFERAFRIEND\", 'DISPLAY': 'OTHER', 'WORKSITE':'OTHER', 'EXTERNAL DATABASE': 'OTHER', 'EXTERNALDATABASE': 'OTHER',  'OTO': 'OTHER'}\n",
    "df_cleaned['distribution_channel'].replace(mapping_distribution, inplace=True)\n",
    "\n",
    "# fill missing values with 0\n",
    "df_cleaned['quote_created_before_count'] = df_cleaned['quote_created_before_count'].fillna(0)\n",
    "df_cleaned['policy_issued_nfc_count'] = df_cleaned['policy_issued_nfc_count'].fillna(0)\n",
    "df_cleaned['scv_policy_issued_nfc_count'] = df_cleaned['scv_policy_issued_nfc_count'].fillna(0)\n",
    "df_cleaned['scv_policy_cancelled_count'] = df_cleaned['scv_policy_cancelled_count'].fillna(0)\n",
    "df_cleaned['scv_policy_inforce_count'] = df_cleaned['scv_policy_inforce_count'].fillna(0)\n",
    "df_cleaned['scv_policy_gi_count'] = df_cleaned['scv_policy_gi_count'].fillna(0)\n",
    "df_cleaned['scv_policy_li_count'] = df_cleaned['scv_policy_li_count'].fillna(0)\n",
    "df_cleaned['quotation_promo_code_value'] = df_cleaned['quotation_promo_code_value'].fillna(0)\n",
    "df_cleaned['since_first_quote'] = df_cleaned['since_first_quote'].fillna(-1)\n",
    "df_cleaned['since_latest_quote'] = df_cleaned['since_latest_quote'].fillna(-1)\n",
    "df_cleaned['since_first_policy'] = df_cleaned['since_first_policy'].fillna(-1)\n",
    "df_cleaned['since_latest_policy'] = df_cleaned['since_latest_policy'].fillna(-1)\n",
    "df_cleaned['since_first_interaction'] = df_cleaned['since_first_interaction'].fillna(-1000)\n",
    "df_cleaned['since_last_interaction'] = df_cleaned['since_last_interaction'].fillna(-1000)\n",
    "\n",
    "# date_count features\n",
    "df_cleaned['avg_days_between_quote'] = (df_cleaned['since_first_quote']/df_cleaned['quote_created_before_count']).fillna(-1)\n",
    "df_cleaned['avg_days_between_policy'] = (df_cleaned['since_first_policy']/df_cleaned['scv_policy_issued_nfc_count']).fillna(-1)\n",
    "\n",
    "# capped age\n",
    "df_cleaned['age_at_quote'] = np.where(df_cleaned['age_at_quote']<18,18,(np.where(df_cleaned['age_at_quote']>65,65,df_cleaned['age_at_quote'])))\n",
    "df_cleaned['age_at_quote'] = df_cleaned['age_at_quote'].fillna(0)\n",
    "\n",
    "# drop non-useful columns ### CAN INVESTIGATE FURTHER\n",
    "df_cleaned = df_cleaned.drop(columns=['distribution_channel', 'event_id', 'nric', 'applicant_marital_status', 'applicant_occupation', 'applicant_nationality', 'issued_before_interaction_flag', 'stp_flag', 'nric_quote', 'matched_quote', 'nric_policy', 'matched_policy'])\n",
    "# df_cleaned = df_cleaned.drop(columns=['quote_valid_flag',])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['product_type', 'device_type', 'applicant_gender', 'nric_type']\n",
    "for i in CATEGORICAL_COLUMNS:\n",
    "    df_cleaned = clip_by_prop(df_cleaned, i, 0.05)\n",
    "    \n",
    "df_cleaned = pd.get_dummies(df_cleaned, columns = CATEGORICAL_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83286c",
   "metadata": {},
   "source": [
    "# Save Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSISTED_PATH = \"./Data/Assisted_Data\"\n",
    "df_cleaned.to_csv(\"{}/df_combined.csv\".format(ASSISTED_PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
